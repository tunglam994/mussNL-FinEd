{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "e4bf7a8e",
   "metadata": {},
   "source": [
    "# Preparing preprocessors and training\n",
    "\n",
    "This notebook splits the train_model.py file into a cell which prepares the preprocessors and a cell which prepares and trains the mbart model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "ce058ce8",
   "metadata": {},
   "outputs": [
    {
     "ename": "ModuleNotFoundError",
     "evalue": "No module named 'easse'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mModuleNotFoundError\u001b[0m                       Traceback (most recent call last)",
      "\u001b[1;32m~\\AppData\\Local\\Temp/ipykernel_27056/3587221461.py\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m      1\u001b[0m \u001b[1;32mimport\u001b[0m \u001b[0mshutil\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      2\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m----> 3\u001b[1;33m \u001b[1;32mfrom\u001b[0m \u001b[0mmuss\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mmining\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mtraining\u001b[0m \u001b[1;32mimport\u001b[0m \u001b[0mget_bart_kwargs\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mget_score_rows\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mget_mbart_kwargs\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m      4\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      5\u001b[0m \u001b[1;32mfrom\u001b[0m \u001b[0mmuss\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mfairseq\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mmain\u001b[0m \u001b[1;32mimport\u001b[0m \u001b[0mcheck_dataset\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mcheck_and_resolve_args\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mprepare_exp_dir\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\Documents\\mussNL\\muss\\mining\\training.py\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m      9\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     10\u001b[0m \u001b[1;32mfrom\u001b[0m \u001b[0mcachier\u001b[0m \u001b[1;32mimport\u001b[0m \u001b[0mcachier\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 11\u001b[1;33m \u001b[1;32mfrom\u001b[0m \u001b[0measse\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mcli\u001b[0m \u001b[1;32mimport\u001b[0m \u001b[0mevaluate_system_output\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     12\u001b[0m \u001b[1;32mfrom\u001b[0m \u001b[0measse\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mutils\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mconstants\u001b[0m \u001b[1;32mimport\u001b[0m \u001b[0mTEST_SETS_PATHS\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     13\u001b[0m \u001b[1;32mimport\u001b[0m \u001b[0mtorch\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mModuleNotFoundError\u001b[0m: No module named 'easse'"
     ]
    }
   ],
   "source": [
    "import shutil\n",
    "import random\n",
    "import shlex\n",
    "from pathlib import Path\n",
    "import re\n",
    "\n",
    "from muss.mining.training import get_bart_kwargs, get_score_rows, get_mbart_kwargs\n",
    "from muss.utils.training import clear_cuda_cache\n",
    "\n",
    "from muss.fairseq.main import check_dataset, check_and_resolve_args, prepare_exp_dir\n",
    "from muss.fairseq.base import fairseq_preprocess, fairseq_train, get_fairseq_exp_dir\n",
    "from muss.preprocessors import get_preprocessors\n",
    "from muss.resources.datasets import create_preprocessed_dataset\n",
    "from muss.resources.paths import get_data_filepath, get_dataset_dir\n",
    "\n",
    "from muss.utils.helpers import log_std_streams, mock_cli_args, print_running_time\n",
    "from fairseq_cli import preprocess, train, generate\n",
    "\n",
    "from muss.utils.helpers import (\n",
    "    log_std_streams,\n",
    "    lock_directory,\n",
    "    create_directory_or_skip,\n",
    "    yield_lines,\n",
    "    write_lines,\n",
    "    mock_cli_args,\n",
    "    create_temp_dir,\n",
    "    mute,\n",
    "    args_dict_to_str,\n",
    ")\n",
    "\n",
    "from muss.text import remove_multiple_whitespaces"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "de0b7561",
   "metadata": {},
   "source": [
    "### Give the name of the dataset to be used"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c2b5091b",
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset = 'uts_nl_query-9fcb6f786a1339d290dde06e16935402_db-9fcb6f786a1339d290dde06e16935402_topk-8_nprobe-16_density-0.6_distance-0.05_filter_ne-False_levenshtein-0.2_simplicity-0.0'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "140a3fdd",
   "metadata": {},
   "outputs": [],
   "source": [
    "kwargs = get_mbart_kwargs(dataset=dataset, language='nl', use_access=True)\n",
    "kwargs['train_kwargs']['ngpus'] = 1  # Set this from 8 to 1 for local training\n",
    "kwargs['train_kwargs']['max_tokens'] = 512  # Lower this number to prevent OOM"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ebf48256",
   "metadata": {},
   "source": [
    "### Define function to prepare the preprocessors and prepare the dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d6e200a1",
   "metadata": {},
   "outputs": [],
   "source": [
    "def prepare_preprocessors_datasets(dataset, **kwargs):\n",
    "    check_dataset(dataset)\n",
    "    kwargs = check_and_resolve_args(kwargs)\n",
    "    exp_dir = prepare_exp_dir()\n",
    "    preprocessors_kwargs = kwargs.get('preprocessors_kwargs', {})\n",
    "    preprocessors = get_preprocessors(preprocessors_kwargs)\n",
    "    if len(preprocessors) > 0:\n",
    "        dataset = create_preprocessed_dataset(dataset, preprocessors, n_jobs=8)\n",
    "        dataset_dir = get_dataset_dir(dataset)\n",
    "        shutil.copy(dataset_dir / 'preprocessors.pickle', exp_dir)\n",
    "        if hasattr(preprocessors[-1], 'copy_sentencepiece_files_to_dir'):\n",
    "            preprocessors[-1].copy_sentencepiece_files_to_dir(dataset_dir)\n",
    "    model_symlink_path = exp_dir / 'model.pt'\n",
    "    if not model_symlink_path.exists():\n",
    "        model_symlink_path.symlink_to('checkpoints/checkpoint_best.pt')\n",
    "    preprocessed_dir = fairseq_preprocess(dataset, **kwargs.get('preprocess_kwargs', {}))\n",
    "    train_kwargs = kwargs.get('train_kwargs', {})\n",
    "    \n",
    "    return preprocessed_dir, exp_dir, train_kwargs"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "88470ae9",
   "metadata": {},
   "source": [
    "### Define function to train the model with fairseq"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d5a1adcc",
   "metadata": {},
   "outputs": [],
   "source": [
    "@clear_cuda_cache\n",
    "def fairseq_train(\n",
    "    preprocessed_dir,\n",
    "    exp_dir,\n",
    "    ngpus=1,\n",
    "    batch_size=8192,  # Batch size across all gpus (taking update freq into account)\n",
    "    max_sentences=64,  # Max sentences per GPU\n",
    "    arch='transformer',\n",
    "    save_interval_updates=100,\n",
    "    max_update=50000,\n",
    "    lr=0.001,\n",
    "    warmup_updates=4000,\n",
    "    dropout=0.1,\n",
    "    lr_scheduler='inverse_sqrt',\n",
    "    criterion='label_smoothed_cross_entropy',\n",
    "    seed=None,\n",
    "    fp16=True,\n",
    "    **kwargs,\n",
    "):\n",
    "    with log_std_streams(exp_dir / 'fairseq_train.stdout'):\n",
    "        exp_dir = Path(exp_dir)\n",
    "        preprocessed_dir = Path(preprocessed_dir)\n",
    "        exp_dir.mkdir(exist_ok=True, parents=True)\n",
    "        # Copy dictionaries to exp_dir for generation\n",
    "        for dict_path in preprocessed_dir.glob('dict.*.txt'):\n",
    "            shutil.copy(dict_path, exp_dir)\n",
    "        checkpoints_dir = exp_dir / 'checkpoints'\n",
    "        total_real_batch_size = max_sentences * ngpus\n",
    "        update_freq = int(round(batch_size / total_real_batch_size, 0))\n",
    "        if seed is None:\n",
    "            seed = random.randint(0, 1000)\n",
    "        distributed_port = random.randint(10000, 20000)\n",
    "        args = f'''\n",
    "        {preprocessed_dir} --task translation --source-lang complex --target-lang simple --save-dir {checkpoints_dir}\n",
    "        --optimizer adam --adam-betas '(0.9, 0.98)' --clip-norm 0.0\n",
    "        --criterion {criterion} --label-smoothing 0.1\n",
    "        --lr-scheduler {lr_scheduler} --lr {lr} --warmup-updates {warmup_updates} --update-freq {update_freq}\n",
    "        --arch {arch} --dropout {dropout} --weight-decay 0.0 --clip-norm 0.1 --share-all-embeddings\n",
    "        --no-epoch-checkpoints --save-interval 999999 --validate-interval 999999\n",
    "        --max-update {max_update} --save-interval-updates {save_interval_updates} --keep-interval-updates 1 --patience 10\n",
    "        --batch-size {max_sentences} --seed {seed}\n",
    "        --distributed-world-size {ngpus} --distributed-port {distributed_port}\n",
    "        '''\n",
    "        if lr_scheduler == 'inverse_sqrt':\n",
    "            args += '--warmup-init-lr 1e-07'\n",
    "        if fp16:\n",
    "            args += f' --fp16'\n",
    "        # FIXME: if the kwargs are already present in the args string, they will appear twice but fairseq will take only the last one into account\n",
    "        args += f' {args_dict_to_str(kwargs)}'\n",
    "        args = remove_multiple_whitespaces(args.replace('\\n', ' ')).strip(' ')\n",
    "        # Recover lost quotes around adam betas\n",
    "        args = re.sub(r'--adam-betas (\\(0\\.\\d+, 0\\.\\d+\\))', r\"--adam-betas '\\1'\", args)\n",
    "        print(f'fairseq-train {args}')\n",
    "        with mock_cli_args(shlex.split(args)):\n",
    "            train.cli_main()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bbf5a799",
   "metadata": {},
   "source": [
    "### Call functions"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "76354b75",
   "metadata": {},
   "source": [
    "#### Prepare"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "58d7e7ac",
   "metadata": {},
   "outputs": [],
   "source": [
    "preprocessed_dir, exp_dir, train_kwargs = print_running_time(prepare_preprocessors_datasets)(**kwargs)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "01b15ecd",
   "metadata": {},
   "source": [
    "#### Train"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d05a660c",
   "metadata": {},
   "outputs": [],
   "source": [
    "print_running_time(fairseq_train)(preprocessed_dir, exp_dir=exp_dir, **train_kwargs)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
